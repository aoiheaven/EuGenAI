# Reinforcement Learning Configuration
# Stage 3: RL-based CoT generation and refinement

# Model Configuration
model:
  image_encoder_name: "vit_base_patch16_224"
  text_encoder_name: "bert-base-uncased"
  img_size: 512
  hidden_dim: 768
  num_cot_steps: 10
  num_heads: 8
  num_decoder_layers: 3
  dropout: 0.1
  num_diagnosis_classes: 100
  
  # Load from weak-supervised checkpoint
  load_pretrained_model: true
  pretrained_checkpoint: "checkpoints_weak_supervised/best_model.pth"
  
  # RL-specific components
  enable_policy_network: true  # Policy for CoT action selection
  enable_value_network: true  # Value estimation for RL
  enable_reward_model: false  # Optional: learned reward model

# Dataset Configuration
dataset:
  data_root: "data"
  train_file: "data/train_weak_labels.json"  # Still only needs diagnosis
  val_file: "data/val_labeled.json"  # Validation with full annotations
  image_size: 512
  max_text_length: 512
  
  require_diagnosis: true
  require_cot: false  # Will be generated by RL agent
  require_segmentation: false
  
  # RL experience buffer
  use_replay_buffer: true
  buffer_size: 10000
  augment_train: true
  augment_val: false

# Reinforcement Learning Configuration
reinforcement_learning:
  # Algorithm
  algorithm: "PPO"  # 'PPO', 'A2C', or 'REINFORCE'
  
  # PPO specific
  ppo:
    clip_epsilon: 0.2
    value_loss_coef: 0.5
    entropy_coef: 0.01
    max_grad_norm: 0.5
    n_epochs_per_update: 4
    batch_size: 64
    minibatch_size: 16
    
  # Experience collection
  experience:
    rollout_steps: 10  # Number of CoT steps
    num_parallel_envs: 4  # Parallel environments
    gae_lambda: 0.95  # GAE parameter
    gamma: 0.99  # Discount factor
    
  # Exploration
  exploration:
    strategy: "epsilon_greedy"  # 'epsilon_greedy' or 'entropy_bonus'
    epsilon_start: 0.3
    epsilon_end: 0.05
    epsilon_decay: 0.995

# Reward Function Design
reward:
  # Multi-component reward
  components:
    # Task performance rewards
    diagnosis_accuracy: 
      weight: 1.0
      type: "sparse"  # Reward at end of episode
      
    attention_localization:
      weight: 0.5
      type: "dense"  # Reward at each step
      method: "overlap_with_saliency"
      
    reasoning_coherence:
      weight: 0.3
      type: "dense"
      method: "perplexity"  # Lower perplexity = more coherent
      
    # Intermediate rewards
    region_relevance:
      weight: 0.4
      type: "dense"
      method: "attention_entropy"  # Lower entropy = more focused
      
    step_diversity:
      weight: 0.2
      type: "dense"
      method: "region_overlap"  # Penalize repetitive regions
      
  # Reward shaping
  shaping:
    normalize: true
    clip_range: [-1, 1]
    use_advantage: true
    
  # Expert demonstration (optional)
  expert_guidance:
    enabled: false
    expert_data_ratio: 0.1  # 10% expert demonstrations
    imitation_weight: 0.3

# Training Configuration
training:
  batch_size: 4  # Smaller for RL
  num_epochs: 200  # RL needs more epochs
  learning_rate: 3.0e-5  # Lower LR for RL stability
  weight_decay: 1.0e-6
  warmup_epochs: 10
  gradient_clip: 0.5  # Tighter clipping for RL
  
  optimizer: "adamw"
  betas: [0.9, 0.999]
  
  scheduler: "cosine"
  min_lr: 1.0e-7
  
  # Update frequency
  policy_update_frequency: 4  # Update policy every N rollouts
  value_update_frequency: 1  # Update value every rollout
  
  # Loss weights
  loss_weights:
    policy_loss: 1.0
    value_loss: 0.5
    entropy_bonus: 0.01
    diagnosis_loss: 0.5  # Auxiliary supervised loss
    kl_divergence: 0.01  # KL penalty for stable updates

# Policy Network Configuration
policy:
  # Action space
  action_space:
    type: "discrete"  # Discrete actions for CoT steps
    num_actions: 100  # Number of possible reasoning actions
    
  # Policy architecture
  architecture:
    hidden_dims: [512, 256]
    activation: "relu"
    use_lstm: true  # LSTM for sequential decision-making
    lstm_hidden_size: 256
    
  # Action selection
  action_selection:
    temperature: 1.0
    top_k: 10  # Sample from top-k actions
    use_beam_search: false

# Value Network Configuration
value:
  architecture:
    hidden_dims: [512, 256]
    activation: "relu"
    use_shared_encoder: true  # Share encoder with policy

# Validation Configuration
validation:
  val_interval: 5
  save_best: true
  metric: "episode_reward"  # RL-specific metric
  compute_return_statistics: true
  
  # Evaluation mode
  eval_mode: "greedy"  # Use greedy policy for evaluation

# Logging Configuration
logging:
  log_dir: "logs_reinforcement_learning"
  tensorboard: true
  wandb: true  # Recommended for RL tracking
  wandb_project: "eugenai-rl"
  log_interval: 5
  
  # RL-specific logging
  log_episode_rewards: true
  log_policy_entropy: true
  log_value_estimates: true
  log_action_distribution: true
  log_cot_trajectories: true
  num_trajectory_samples: 3

# Checkpoint Configuration
checkpoint:
  save_dir: "checkpoints_reinforcement_learning"
  save_interval: 10
  keep_last_n: 5
  save_best_reward: true

# Hardware Configuration
hardware:
  device: "cuda"
  num_workers: 4
  pin_memory: true
  mixed_precision: true

# Reproducibility
seed: 42

# Curriculum Learning (optional)
curriculum:
  enabled: true
  start_num_steps: 3  # Start with 3-step CoT
  end_num_steps: 10  # Gradually increase to 10
  increase_interval: 20  # Increase every N epochs
  
# Notes:
# - This config is for Stage 3: RL-based CoT generation
# - The model learns to generate reasoning chains through trial and error
# - Reward function guides the model to produce accurate, coherent, and localized reasoning
# - After RL training, optionally fine-tune with small amount of expert CoT data (Stage 4)

